<?xml version="1.0" encoding="UTF-8" standalone="no"?><opml version="2.0"><head><title>Chapter 9 Up and Running with Tensorflow</title><owername>Administrator</owername><producer>XMind</producer><xmind-version>3.7.8</xmind-version></head><body><outline text="Chapter 9 Up and Running with Tensorflow"><outline text="tensorflow的优势"><outline text="python API: tensorflow.contrib.learn"/><outline text="简化的python API: tensorflow.contrib.slim"/><outline text="几乎你能想到的任何神经网络结构"/><outline text="C++ API"/><outline text="谷歌云服务"/><outline text="社区：github/jtoy/awesome-tensorflow"/></outline><outline text="建立第一张Graph在Session中运行"><outline text="建立图"><outline text="x = tf.Variable(3, name=&quot;x&quot;)&#13;&#10;y = tf.Variable(4, name=&quot;y&quot;)&#13;&#10;f = x*x*y + y +2"/></outline><outline text="在sess中initialize和run"><outline text="sess = tf.Session()&#13;&#10;sess.run(x.initializer)&#13;&#10;sess.run(y.initializer)&#13;&#10;result = sess.run(f)&#13;&#10;print(result)           #42&#13;&#10;sess.close()"/><outline text="简易写法"><outline text="with tf.Session() as sess: # 把sess设定为默认的session,而且会自动关闭session&#13;&#10;&#9;x.initializer.run()    # 等价于tf.get_default_session().run(x.initializer)&#13;&#10;&#9;y.initializer.run()    &#13;&#10;&#9;result = f.eval()      # 等价于tf.get_default_session().run(f)"/></outline><outline text="global_variables_initializer()"><outline text="这个不是马上初始化所有变量Variable，而是创建一个节点当run这个节点的时候进行初始化"/><outline text="init = tf.global_variables_initializer()&#13;&#10;&#13;&#10;with tf.Session() as sess:&#13;&#10;&#9;init.run()&#13;&#10;&#9;result = f.eval() # 计算f！"/></outline><outline text="自己设自己为默认session:&#13;&#10;InteractiveSession()"><outline text="sess = tf.InteractiveSession()&#13;&#10;init.run()&#13;&#10;result = f.eval()&#13;&#10;sess.close() # 还是要手动关闭session"/></outline></outline></outline><outline text="管理graph"><outline text="创建的任何节点都会加入到默认的graph中"><outline text="x1 = tf.Variable(1)&#13;&#10;x1.graph is tf.get_default_graph()  # True"/></outline><outline text="可以管理过个独立的graph"><outline text="graph = tf.Graph()&#13;&#10;with graph.as_default(): # 作为临时的default graph&#13;&#10;&#9;x2 = tf.Variable(2)&#13;&#10;&#13;&#10;x2.graph is tf_default_graph()  # False 出了with就不是默认的graph了"/></outline><outline text="tf.reset_default_graph()或者重启kernel(shell)"><outline text="在jupyter或者IDE中，有时会重复地在默认graph中添加node，我们要在每次执行前reset默认graph（清空）"/></outline></outline><outline text="node和variable的生命周期"><outline text="每次sess.run(也叫graph run）的时候，都把图重新跑一遍，即使他们共享一些值"><outline text="w = tf.constant(3)&#13;&#10;x = w + 2&#13;&#10;y = x + 5&#13;&#10;z = x * 3&#13;&#10;&#13;&#10;with tf.Session() as sess:&#13;&#10;&#9;print(y.eval())  # 10&#13;&#10;&#9;print(z.eval())  # 15"><outline text="求y和z的时候都会从头把x和w求一遍，先确定y和z的依赖"/></outline></outline><outline text="如果要更高效地计算y和z，也就是在不同的graph run中分享共同依赖的节点"><outline text="with tf.Session() as sess:&#13;&#10;&#9;y_val, z_val = sess.run([y, z]) # 在一次graph run中同时求y和z&#13;&#10;&#9;print(y_val)&#13;&#10;&#9;print(z.val)"/></outline><outline text="node的生命周期只有一次graph run&#13;&#10;variable的什么周期在初始化时开始，session结束时结束（如果再要用需要重新初始化）"/></outline><outline text="线性回归"><outline text="tensor是什么"><outline text="输入和输出都是多维array，所以叫tensor"/></outline><outline text="&#13;&#10;import numpy as np&#13;&#10;from sklearn.datasets import fetch_california_housing&#13;&#10;&#13;&#10;reset_graph()&#13;&#10;&#13;&#10;housing = fetch_california_housing()&#13;&#10;m, n = housing.data.shape&#13;&#10;housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] &#13;&#10;&#13;&#10;X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=&quot;X&quot;)&#13;&#10;y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=&quot;y&quot;) #本来target只是个一维的向量，要让它变为2维的，才能相乘。&#13;&#10;XT = tf.transpose(X)&#13;&#10;theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)&#13;&#10;&#13;&#10;with tf.Session() as sess:&#13;&#10;    theta_value = theta.eval()"/></outline><outline text="实施梯度下降Gradient Descent"><outline text="手动计算梯度"><outline text="reset_graph()&#13;&#10;&#13;&#10;n_epochs = 1000&#13;&#10;learning_rate = 0.01&#13;&#10;&#13;&#10;X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=&quot;X&quot;)&#13;&#10;y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=&quot;y&quot;)&#13;&#10;theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=&quot;theta&quot;)&#13;&#10;y_pred = tf.matmul(X, theta, name=&quot;predictions&quot;)&#13;&#10;error = y_pred - y&#13;&#10;mse = tf.reduce_mean(tf.square(error), name=&quot;mse&quot;)&#13;&#10;gradients = 2/m * tf.matmul(tf.transpose(X), error)&#13;&#10;training_op = tf.assign(theta, theta - learning_rate * gradients)&#13;&#10;&#13;&#10;init = tf.global_variables_initializer()&#13;&#10;&#13;&#10;with tf.Session() as sess:&#13;&#10;    sess.run(init)&#13;&#10;&#13;&#10;    for epoch in range(n_epochs):&#13;&#10;        if epoch % 100 == 0:&#13;&#10;            print(&quot;Epoch&quot;, epoch, &quot;MSE =&quot;, mse.eval())&#13;&#10;        sess.run(training_op)&#13;&#10;    &#13;&#10;    best_theta = theta.eval() #为了让它返回一个tensor值"/></outline></outline></outline></body></opml>