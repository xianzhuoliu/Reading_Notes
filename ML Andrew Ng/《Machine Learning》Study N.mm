<map>
		<node ID="root" TEXT="《Machine Learning》Study Notes">		<node TEXT="Lecture 1 Introduction" ID="be167d663433b126" STYLE="bubble" POSITION="right">
		<node TEXT="1. What is Machine Learning" ID="d0167d663433e181" STYLE="fork">
		<node TEXT="1.1 Definition" ID="30a167d663433f144" STYLE="fork">
		</node>
		<node TEXT="1.2 Types of learning algorithms" ID="3dd167d6634340061" STYLE="fork">
		</node>
		</node>
		<node TEXT="2. Supervised learning - introduction" ID="341167d6634341087" STYLE="fork">
		<node TEXT="2.1 Regression Problem" ID="30d167d66343410821" STYLE="fork">
		</node>
		<node TEXT="2.2 Classification problem" ID="3aa167d6634342195" STYLE="fork">
		</node>
		</node>
		<node TEXT="3. Unsupervised learning" ID="35167d66343421091" STYLE="fork">
		<node TEXT="3.1 clustering algorithm" ID="10e167d663434211e2" STYLE="fork">
		</node>
		<node TEXT="3.2 Cocktail party algorithm" ID="32b167d6634342173" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="Lecture 2 Linear Regression with One Variable" ID="3d6167d663434207b4" STYLE="bubble" POSITION="right">
		<node TEXT="1. Model Representation" ID="34e167d66343430f2" STYLE="fork">
		</node>
		<node TEXT="2. Cost Function" ID="3af167d663434e0fb" STYLE="fork">
		</node>
		<node TEXT="3. Gradient Descent" ID="3a5167d663435117c" STYLE="fork">
		<node TEXT="3.1 How does it work?" ID="32e167d66343511611" STYLE="fork">
		</node>
		<node TEXT="3.2 Formal definition" ID="86167d663435105b2" STYLE="fork">
		</node>
		</node>
		<node TEXT="4. Linear regression with gradient descent" ID="11a167d6634352129" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 4 Linear Regression with Multiple Variables" ID="2f0167d663435200c1" STYLE="bubble" POSITION="right">
		<node TEXT="1. Multiple features" ID="1d3167d663435404b" STYLE="fork">
		<node TEXT="1.1 Notations" ID="221167d66343540741" STYLE="fork">
		</node>
		<node TEXT="1.2 Formula" ID="12a167d663435415e2" STYLE="fork">
		</node>
		</node>
		<node TEXT="2. Gradient descent for multiple variables" ID="119167d663435502f" STYLE="fork">
		<node TEXT="2.1 cost function and gradient descent" ID="241167d66343551341" STYLE="fork">
		</node>
		<node TEXT="2.2 Gradient Decent in practice: 1 Feature Scaling" ID="e8167d663435507e2" STYLE="fork">
		</node>
		<node TEXT="2.3 Gradient Decent in practice: 2 Learning Rate" ID="1d3167d6634356055" STYLE="fork">
		<node TEXT="2.3.1 Iteration" ID="73167d6634357076" STYLE="fork">
		</node>
		<node TEXT="2.3.2 J is increasing" ID="365167d6634357021" STYLE="fork">
		</node>
		<node TEXT="2.3.3 J looks like waves" ID="34e167d663435813c" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="3. Features and polynomial regression" ID="238167d66343580a81" STYLE="fork">
		<node TEXT="3.1 New features" ID="2ec167d66343580e82" STYLE="fork">
		</node>
		<node TEXT="3.2 Polynomial regression" ID="ab167d663435905e" STYLE="fork">
		</node>
		</node>
		<node TEXT="4. Normal Equation" ID="14a167d663435a10e" STYLE="fork">
		<node TEXT="4.1 How does it work" ID="1c1167d663435a08c1" STYLE="fork">
		</node>
		<node TEXT="4.2 Example" ID="390167d663435b193" STYLE="fork">
		</node>
		<node TEXT="4.3 Gradient descent VS Normal Equation" ID="235167d663435b18a1" STYLE="fork">
		<node TEXT="4.3.1 Gradient descent" ID="99167d663435c0ca" STYLE="fork">
		</node>
		<node TEXT="4.3.2 Normal Equation" ID="163167d663435c0ab1" STYLE="fork">
		</node>
		</node>
		<node TEXT="4.4 What if (X^T X) is non-invertible" ID="5167d663435c2" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="Lecture 6 Logistic Regression" ID="154167d663435d083" STYLE="bubble" POSITION="right">
		<node TEXT="1. what is Logistic Regression" ID="390167d663435e16" STYLE="fork">
		</node>
		<node TEXT="2. Hypothesis representation" ID="3dd167d663435e0fe1" STYLE="fork">
		</node>
		<node TEXT="3. Decision boundary" ID="29167d663435e1552" STYLE="fork">
		<node TEXT="3.1 linear decision boundaries" ID="307167d663435e0323" STYLE="fork">
		</node>
		<node TEXT="3.2 Non-linear decision boundaries" ID="1b1167d663435f0fd" STYLE="fork">
		</node>
		</node>
		<node TEXT="4. Cost Function" ID="352167d663436011d" STYLE="fork">
		<node TEXT="4.1 why not cost function in linear regression" ID="352167d66343601221" STYLE="fork">
		</node>
		<node TEXT="4.2 A convex logistic regression cost function" ID="16c167d66343610d8" STYLE="fork">
		</node>
		</node>
		<node TEXT="5. Simplified cost function and gradient descent" ID="256167d6634362139" STYLE="fork">
		<node TEXT="5.1 How to minimize the logistic regression cost function" ID="376167d66343621941" STYLE="fork">
		</node>
		</node>
		<node TEXT="6. Advanced Optimization" ID="12d167d66343630c6" STYLE="fork">
		</node>
		<node TEXT="7. Multiclass classification problems" ID="33a167d663436404b" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 7 Regularization" ID="11167d6634367089" STYLE="bubble" POSITION="right">
		<node TEXT="1. The problem of overfitting" ID="3c4167d6634368038" STYLE="fork">
		<node TEXT="1.1 Underfitting (high bias)" ID="3cb167d663436902d" STYLE="fork">
		</node>
		<node TEXT="1.2 Overfitting (high variance)" ID="17167d663436914c1" STYLE="fork">
		<node TEXT="1.2.1 Addressing overfitting" ID="13f167d663436a176" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="2. Cost function optimization for regularization" ID="1f8167d663436a0ed1" STYLE="fork">
		</node>
		<node TEXT="3. Regularized linear regression and logistic regression" ID="fd167d663436b076" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 8 Neural Networks - Representation" ID="35f167d663437010a" STYLE="bubble" POSITION="right">
		<node TEXT="1. Why do we need neural networks?" ID="a4167d66343701061" STYLE="fork">
		</node>
		<node TEXT="2. Model representation" ID="ab167d663437006b2" STYLE="fork">
		</node>
		<node TEXT="3. Neural network example" ID="1bf167d663437014e3" STYLE="fork">
		</node>
		<node TEXT="4. Multiclass classification" ID="19b167d663437d043" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 9 Neural Network: Learning" ID="11b167d663437e11c" STYLE="bubble" POSITION="right">
		<node TEXT="1. Cost Function" ID="2d9167d663437e0591" STYLE="fork">
		</node>
		<node TEXT="2. Back propagation algorithm" ID="380167d663437e01b2" STYLE="fork">
		</node>
		<node TEXT="3. Back propagation intuition" ID="3e6167d663437e09d3" STYLE="fork">
		</node>
		<node TEXT="4. Gradient Checking" ID="1d4167d663437f16b" STYLE="fork">
		</node>
		<node TEXT="5. Random initialization" ID="d6167d663437f0e11" STYLE="fork">
		</node>
		<node TEXT="6. Summary" ID="3bc167d66343810a3" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 10 Advice for applying Machine Learning" ID="67167d66343810ac1" STYLE="bubble" POSITION="left">
		<node TEXT="1. Deciding what to try next" ID="13a167d66343810572" STYLE="fork">
		</node>
		<node TEXT="2. Evaluating a hypothesis" ID="38c167d66343810a33" STYLE="fork">
		</node>
		<node TEXT="3. Model selection and training validation test sets" ID="27f167d66343820d4" STYLE="fork">
		</node>
		<node TEXT="4. Diagnosis - bias vs. variance" ID="18167d6634385035" STYLE="fork">
		</node>
		<node TEXT="5. Regularization and bias/variance" ID="19b167d66343850ea1" STYLE="fork">
		</node>
		<node TEXT="5. learning curve" ID="8e167d66343860c6" STYLE="fork">
		</node>
		<node TEXT="6. What to do next (revisited)" ID="1f5167d66343861861" STYLE="fork">
		<node TEXT="6.2 network architecture" ID="b4167d6634387099" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="Lecture 11 Machine Learning System Design" ID="115167d66343871921" STYLE="bubble" POSITION="left">
		<node TEXT="1. Prioritizing what to work on--represent features" ID="181167d663438706f2" STYLE="fork">
		</node>
		<node TEXT="2. Error analysis" ID="29167d663438807a" STYLE="fork">
		</node>
		<node TEXT="3. Error metrics for skewed analysis" ID="1e7167d6634389077" STYLE="fork">
		<node TEXT="3.1 Precision and recall" ID="99167d66343890ba1" STYLE="fork">
		</node>
		<node TEXT="3.2 Trading off precision and recall" ID="25c167d663438903d2" STYLE="fork">
		</node>
		</node>
		<node TEXT="4. Large data rational" ID="34c167d663438916d3" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 12 Support vector machine" ID="ec167d663438b08e" STYLE="bubble" POSITION="left">
		<node TEXT="1. Optimizaiton Object" ID="16167d663438b1021" STYLE="fork">
		</node>
		<node TEXT="2. Large margin intuition" ID="64167d663438b1122" STYLE="fork">
		</node>
		<node TEXT="3. Large margin classification mathematics" ID="30a167d663438c108" STYLE="fork">
		<node TEXT="3.1 Inner product" ID="363167d663438d042" STYLE="fork">
		</node>
		<node TEXT="3.2 SVM decision boundary" ID="32167d663438d04e1" STYLE="fork">
		</node>
		</node>
		<node TEXT="4. Kernels: Adapting SVM to non-linear classifiers" ID="2e2167d663438e06f" STYLE="fork">
		</node>
		<node TEXT="4.1 Gaussian Kernel" ID="28c167d663438e14c1" STYLE="fork">
		</node>
		<node TEXT="4.2 What does σ do?" ID="12a167d663438e0af2" STYLE="fork">
		</node>
		<node TEXT="4.3 what kinds of hypotheses can we learn?" ID="3b3167d663438f01" STYLE="fork">
		</node>
		<node TEXT="4.4 Implementation of kernels" ID="a9167d6634390032" STYLE="fork">
		<node TEXT="4.4.1 Choosing the landmarks" ID="2e9167d66343901451" STYLE="fork">
		</node>
		<node TEXT="4.4.2 SVM hypothesis prediction with kernels" ID="1cb167d663439114d" STYLE="fork">
		</node>
		<node TEXT="4.4.3 SVM training with kernels" ID="303167d66343910f41" STYLE="fork">
		</node>
		<node TEXT="4.4.4 SVM parameters (C)" ID="15b167d663439201d" STYLE="fork">
		</node>
		<node TEXT="4.4.5 SVM parameters (σ2)" ID="3c4167d6634393158" STYLE="fork">
		</node>
		</node>
		<node TEXT="5. SVM - implementation and use" ID="316167d6634393121" STYLE="fork">
		<node TEXT="5.1 Choosing a kernel" ID="2b6167d66343930792" STYLE="fork">
		</node>
		<node TEXT="5.2 Logistic regression vs. SVM" ID="150167d663439316f3" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="Lecture 13 Cluster" ID="124167d663439410b" STYLE="bubble" POSITION="left">
		<node TEXT="1. Unsupervised learning - introduction" ID="2b8167d66343940e11" STYLE="fork">
		</node>
		<node TEXT="2. K-means algorithm" ID="1b167d66343950bc" STYLE="fork">
		<node TEXT="2.1 Algorithm overview" ID="345167d66343950571" STYLE="fork">
		</node>
		<node TEXT="2.2 K-means for non-separated clusters" ID="3c9167d6634397057" STYLE="fork">
		</node>
		</node>
		<node TEXT="3. K means optimization objective" ID="180167d663439714c1" STYLE="fork">
		</node>
		<node TEXT="4. Random initialization" ID="12167d663439817d" STYLE="fork">
		</node>
		<node TEXT="5. Choose the number of clusters" ID="350167d6634399194" STYLE="fork">
		<node TEXT="5.1 Elbow method" ID="29c167d66343990f41" STYLE="fork">
		</node>
		<node TEXT="5.2 For a later/downstream purpose" ID="348167d663439a10a" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="Lecture 14 Dimensionality Reduc1on" ID="157167d663439a1171" STYLE="bubble" POSITION="left">
		<node TEXT="1. Motivation" ID="200167d663439a0ce2" STYLE="fork">
		<node TEXT="1.1 Motivation 1: Data compression" ID="1f6167d663439b12c" STYLE="fork">
		</node>
		<node TEXT="1.2 Motivation 2: Visualization" ID="fb167d663439b13b1" STYLE="fork">
		</node>
		</node>
		<node TEXT="2. Principle Component Analysis (PCA): Problem Formulation" ID="28167d663439c05e" STYLE="fork">
		<node TEXT="2.1 VS linear regression" ID="1e0167d663439c13b1" STYLE="fork">
		</node>
		<node TEXT="2.2 algorithm" ID="4d167d663439d133" STYLE="fork">
		<node TEXT="algorithm description" ID="368167d663439d06f1" STYLE="fork">
		</node>
		</node>
		</node>
		<node TEXT="3. Reconstruction from Compressed Representation" ID="140167d663439d0962" STYLE="fork">
		</node>
		<node TEXT="4. Choosing the number of Principle Components" ID="31e167d663439e0fe" STYLE="fork">
		</node>
		<node TEXT="5. Advice for Applying PCA" ID="150167d663439f0b" STYLE="fork">
		</node>
		</node>
		<node TEXT="Lecture 15 Anomaly Detection" ID="15c167d663439f0e31" STYLE="bubble" POSITION="left">
		<node TEXT="1. problem motivation" ID="273167d663439f0062" STYLE="fork">
		</node>
		<node TEXT="2. The Gaussian(Normal) distribution" ID="12167d66343a010b" STYLE="fork">
		<node TEXT="algorithm" ID="196167d66343a0011" STYLE="fork">
		</node>
		</node>
		<node TEXT="3. Developing and evaluating and anomaly detection system" ID="3c1167d66343a101c" STYLE="fork">
		</node>
		<node TEXT="4. Anomaly detection vs. supervised learning" ID="268167d66343a10e61" STYLE="fork">
		</node>
		<node TEXT="5. Choosing features to use" ID="2fc167d66343a11062" STYLE="fork">
		<node TEXT="5.1 Non-Gaussian features." ID="1f7167d66343a10e93" STYLE="fork">
		</node>
		<node TEXT="5.2 Error analysis" ID="97167d66343a2017" STYLE="fork">
		</node>
		</node>
		<node TEXT="6. Multivariate Gaussian distribution" ID="1f6167d66343a21811" STYLE="fork">
		<node TEXT="Algorithm" ID="14d167d66343a3065" STYLE="fork">
		</node>
		<node TEXT="Original model vs. Multivariate Gaussian" ID="258167d66343a4019" STYLE="fork">
		</node>
		</node>
		</node>
</node>
</map>